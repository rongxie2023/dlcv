{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rongxie2023/dlcv/blob/main/Chapter02/01-Tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# tensor基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "shape: torch.Size([2, 3])\n",
      "size: torch.Size([2, 3])\n",
      "dim 2\n",
      "torch.int32\n",
      "torch.float32\n",
      "cpu\n",
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# a = torch.tensor([[1, 2, 3], [4, 5, 6]],dtype=torch.float32)\n",
    "\n",
    "a\n",
    "# print(a)\n",
    "# display(a)\n",
    "\n",
    "#形状\n",
    "print(a.reshape(2,3))\n",
    "print(a.reshape(2,-1))\n",
    "print(\"shape:\",a.shape)\n",
    "print(\"size:\",a.size())\n",
    "print(\"dim\",a.dim())\n",
    "\n",
    "#数据类型\n",
    "# torch.int8 16 32 64\n",
    "# torch.float16 32 64\n",
    "\n",
    "#两种数据类型转换方法\n",
    "a = torch.tensor([[3, 4],[1, 2]], dtype=torch.int)\n",
    "b = a.to(torch.float)  \n",
    "c = a.float()  #b.int()\n",
    "print(a.dtype)\n",
    "print(b.dtype)\n",
    "\n",
    "#cpu和gpu的移动\n",
    "a = torch.tensor([[3, 4],[1, 2]]) #默认在cpu\n",
    "# a = torch.tensor([[3, 4],[1, 2]],device=\"cuda:0\") #保存到gpu\n",
    "\n",
    "print(a.device)\n",
    "b = a.to(\"cuda\")\n",
    "print(b.device)\n",
    "c = a.cpu()\n",
    "print(c.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(size=(3, 3))  # Tensor of shape 3x3 with uninitialized data\n",
    "x = torch.zeros((3, 3))  # Tensor of shape 3x3 with values of 0\n",
    "x = torch.rand((3, 3))  # Tensor of shape 3x3 with values from uniform distribution in interval [0,1)\n",
    "x = torch.ones((3, 3))  # Tensor of shape 3x3 with values of 1\n",
    "x = torch.eye(5, 5)  # Returns Identity Matrix I, (I <-> Eye), matrix of shape 2x3\n",
    "x = torch.arange(start=0, end=5, step=1)  # Tensor [0, 1, 2, 3, 4], note, can also do: torch.arange(11)\n",
    "x = torch.linspace(start=0.1, end=1, steps=10)  # x = [0.1, 0.2, ..., 1]\n",
    "x = torch.empty(size=(1, 5)).normal_(mean=0, std=1)  # Normally distributed with mean=0, std=1  正态分布\n",
    "x = torch.empty(size=(1, 5)).uniform_(0, 1)  # Values from a uniform distribution low=0, high=1  均匀分布\n",
    "x = torch.diag(torch.ones(3))  # Diagonal matrix of shape 3x3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 tensor与numpy互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((5, 5))\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "# b=torch.tensor(a)\n",
    "print(b)\n",
    "c= b.numpy()\n",
    "print(type(b))\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5])\n",
      "tensor([1.0000, 1.4142, 1.7321])\n",
      "tensor([5, 7, 9])\n",
      "tensor([4, 5, 6])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n",
      "tensor([False, False, False])\n",
      "tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "\n",
    "#与标量scalar的计算\n",
    "print(x+2)\n",
    "print(x**0.5)\n",
    "\n",
    "# -- 加减乘除 相同位置的元素进行运算--\n",
    "z1 = torch.empty(3)\n",
    "torch.add(x, y, out=z1)  # This is one way\n",
    "z2 = torch.add(x, y)  # This is another way\n",
    "z1 = x + y  # This is my preferred way, simple and clean.\n",
    "z2= x+3\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(x/y)\n",
    "\n",
    "# -- 逻辑运算--\n",
    "print(x>3)\n",
    "print(x<y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Inplace 操作 --\n",
    "t = torch.zeros(3)\n",
    "\n",
    "t.add_(x)  # Whenever we have operation followed by _ it will mutate the tensor in place\n",
    "t += x  # Also inplace: t = t + x is not inplace, bit confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.4142, 1.7321])\n",
      "tensor([0.8415, 0.9093, 0.1411])\n",
      "tensor([0.0000, 0.6931, 1.0986])\n"
     ]
    }
   ],
   "source": [
    "# --数学函数\n",
    "z = x.pow(2)  # z = [1, 4, 9]\n",
    "z = x**2  # z = [1, 4, 9]\n",
    "print(torch.sqrt(x))\n",
    "print(torch.sin(x))\n",
    "print(torch.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  5],\n",
      "        [11, 11]])\n",
      "tensor([[ 5,  5],\n",
      "        [11, 11]])\n",
      "tensor([[1, 2],\n",
      "        [6, 8]])\n",
      "tensor(32)\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# -- 矩阵乘法 --\n",
    "x1 = torch.tensor([[1,2],[3,4]])\n",
    "x2 =  torch.tensor([[1,1],[2,2]])\n",
    "x3 = torch.mm(x1, x2)  # Matrix multiplication of x1 and x2, out shape: 2x3\n",
    "x3 = x1.mm(x2)  # Similar as line above\n",
    "x4 = x1@x2\n",
    "\n",
    "print(x3)\n",
    "print(x4)\n",
    "\n",
    "# -- 元素相同位置乘 --\n",
    "z = x1 * x2\n",
    "print(z)\n",
    "\n",
    "# -- 向量内积 --\n",
    "z = torch.dot(x, y)  # Dot product, in this case z = 1*9 + 2*8 + 3*7\n",
    "print(z)\n",
    "\n",
    "# -- 转置 --\n",
    "print(x1.T)\n",
    "\n",
    "# -- 逆 --\n",
    "print(x1.float().inverse())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "# -- Batch Matrix Multiplication --\n",
    "batch = 32\n",
    "n = 10\n",
    "m = 20\n",
    "p = 30\n",
    "tensor1 = torch.rand((batch, n, m))\n",
    "tensor2 = torch.rand((batch, m, p))\n",
    "out_bmm = torch.bmm(tensor1, tensor2)  # Will be shape: (b x n x p)\n",
    "\n",
    "print(out_bmm.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3],\n",
      "        [3, 3],\n",
      "        [3, 3],\n",
      "        [3, 3]])\n",
      "tensor([[2, 2]])\n",
      "tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.full((4, 2), 3)\n",
    "x2 = torch.full((1, 2),2)\n",
    "z = x1-x2  # Shape of z is 5x5: How? The 1x5 vector (x2) is subtracted for each row in the 5x5 (x1)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 其它有用的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor(21) tensor([5, 7, 9]) tensor([ 6, 15])\n",
      "tensor([1, 2, 3]) tensor([0, 0, 0])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([4, 5, 6])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1])\n",
      "torch.return_types.sort(\n",
      "values=tensor([1, 2, 3, 4, 5, 6]),\n",
      "indices=tensor([0, 1, 2, 3, 4, 5]))\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(1,7).reshape(-1,3)\n",
    "print(x)\n",
    "\n",
    "sum_x1 = torch.sum(x)  # sum all , sum_x = 21\n",
    "sum_x2 = torch.sum(x, dim=0)  # Sum of x across dim=0 x轴, sum_x =[5, 7, 9]\n",
    "sum_x3 = torch.sum(x, dim=1)  # Sum of x across dim=1 y轴, sum_x = [ 6, 15]\n",
    "print(sum_x1,sum_x2,sum_x3)\n",
    "\n",
    "values, indices = torch.max(x, dim=0)  # Can also do x.max(dim=0)\n",
    "values, indices = torch.min(x, dim=0)  # Can also do x.min(dim=0)\n",
    "print(values, indices )\n",
    "\n",
    "abs_x = torch.abs(x)  # Returns x where abs function has been applied to every element\n",
    "z = torch.argmax(x, dim=0)  # Gets index of the maximum value  返回下标\n",
    "z = torch.argmin(x, dim=0)  # Gets index of the minimum value\n",
    "\n",
    "mean_x = torch.mean(x.float(), dim=0)  # mean requires x to be float\n",
    "z = torch.eq(x, y)  # Element wise comparison, in this case z = [False, False, False]\n",
    "sorted_y, indices = torch.sort(y, dim=0, descending=False)\n",
    "\n",
    "z = torch.clamp(x, min=0)  #截断\n",
    "# All values < 0 set to 0 and values > 0 unchanged (this is exactly ReLU function)\n",
    "# If you want to values over max_val to be clamped, do torch.clamp(x, min=min_val, max=max_val)\n",
    "print(z)\n",
    "\n",
    "print(x[1])\n",
    "print(torch.bincount(x[1]))  #计数\n",
    "print(x.reshape(-1).sort()) #排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29, 30]])\n",
      "tensor(1) tensor(1)\n",
      "1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([2, 3, 4, 5, 6])\n",
      "tensor([2, 4, 6])\n",
      "tensor([4, 5, 6])\n",
      "tensor([6, 5, 4, 3, 2, 1])\n",
      "tensor([ 3,  9, 15, 21, 27])\n",
      "tensor([[ 3],\n",
      "        [ 9],\n",
      "        [15],\n",
      "        [21],\n",
      "        [27]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(1,31).reshape(-1,6)\n",
    "print(x)\n",
    "\n",
    "print(x[0,0],x[0][0])  #通过下标访问tensor元素\n",
    "print(x[0,0].item())  #取出元素\n",
    "\n",
    "###切片\n",
    "print(x[0]) #取出一行\n",
    "t=x[0]\n",
    "\n",
    "print(t[1:6])  #前闭后开 [)\n",
    "print(t[1:6:2])  #步长  步长必须大于0\n",
    "print(t[-3:])  #末尾三个\n",
    "print(t.flip(0))  #步长必须大于0, ::-1 这种翻转数组不可以用\n",
    "\n",
    "print(x[:,2]) #取出一列\n",
    "print(x[:,2:3]) #取出一列并保持维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "tensor([2, 3])\n",
      "tensor([[ 1, 88, 88,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "tensor([[ 1, 88, 88, 99],\n",
      "        [ 5,  6,  7, 99],\n",
      "        [ 9, 10, 11, 99]])\n"
     ]
    }
   ],
   "source": [
    "#注意切片和reshape都是视图,不是复制，对视图上的修改，都会影响到源数据\n",
    "x=torch.arange(1,13).reshape(-1,4)\n",
    "print(x)\n",
    "\n",
    "y=x[0,1:3]\n",
    "print(y)\n",
    "y[:]=88\n",
    "print(x)\n",
    "\n",
    "x[:,3]=99\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 花式索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29, 30]])\n",
      "tensor([[ 7,  8,  9, 10, 11, 12],\n",
      "        [25, 26, 27, 28, 29, 30],\n",
      "        [13, 14, 15, 16, 17, 18]])\n",
      "tensor([[ 1,  3,  2],\n",
      "        [ 7,  9,  8],\n",
      "        [13, 15, 14],\n",
      "        [19, 21, 20],\n",
      "        [25, 27, 26]])\n",
      "tensor([ 7, 27, 14])\n",
      "tensor([[ 7,  9,  8],\n",
      "        [25, 27, 26],\n",
      "        [13, 15, 14]])\n"
     ]
    }
   ],
   "source": [
    "#花式索引 fancy index\n",
    "x=torch.arange(1,31).reshape(-1,6)\n",
    "print(x)\n",
    "\n",
    "print( x[[1,4,2]] )  #选出x[1], x[4] x[2]这几行\n",
    "print( x[:,[0,2,1]] ) #选出 0 2 1这几列\n",
    "print( x[[1,4,2],[0,2,1]] )  #选出x[1,0] x[4,2] x[2,1]这几个元素\n",
    "print( x[[1,4,2]][:,[0,2,1]] ) #区域选取，选择1,4,2行的0,2,1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29, 30]])\n",
      "tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29, 30]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0],\n",
      "        [19, 20, 21, 22, 23, 24],\n",
      "        [ 0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "#注意花式索引把数据复制到新数组，即拷贝，不是视图\n",
    "x=torch.arange(1,31).reshape(-1,6)\n",
    "print(x)\n",
    "\n",
    "y=x[[1,4,2]]\n",
    "y[:]=0\n",
    "print(y)\n",
    "print(x)\n",
    "\n",
    "x[[1,4,2]]=0   #这个是inplace操作\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 布尔型索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  4, -2, -5],\n",
      "        [-2,  4,  2, -2],\n",
      "        [ 2, -2, -4,  1],\n",
      "        [ 1,  4,  3,  1]])\n",
      "tensor([4, 4, 2, 2, 1, 1, 4, 3, 1])\n",
      "tensor([ 4, -2, -2,  4, -2, -2, -4,  4,  3])\n",
      "x= tensor([[ 1,  3, -1, -2,  1,  4, -4, -1]])\n",
      "y= tensor([[-1,  2, -2,  2,  0, -3, -2, -3]])\n",
      "tensor([1, 3, 1, 4])\n",
      "tensor([ 3, -2])\n",
      "tensor([[ -1, 999,  -2, 999,   0,  -3,  -2,  -3]])\n",
      "tensor([[ -1, 999,  -2, 888,   0,  -3,  -2,  -3]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # # 设置随机数种子\n",
    "x=torch.randint(-5,5,(4,4))\n",
    "print(x)\n",
    "\n",
    "print(x[ x>0] )\n",
    "print(x[ (x>2) | (x<=-2) & (x>=-4) ])\n",
    "\n",
    "x=torch.randint(-5,5,(1,8))\n",
    "y=torch.randint(-3,3,(1,8))\n",
    "print(\"x=\",x)\n",
    "print(\"y=\",y)\n",
    "print(x[x>0])  #筛选数据  选择x中大于0的元素\n",
    "print(x[y>0])  #筛选数据  选择x中 y>0的位置相同的元素\n",
    "\n",
    "y[y>0]=999  #筛选并修改\n",
    "print(y)\n",
    "y[y>0]=torch.tensor([999,888])  #在相应位置进行修改\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变化形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([64, 2, 5]) torch.Size([64, 5, 2])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([10, 1])\n",
      "Original Tensor Shape: torch.Size([1, 5, 1, 1])\n",
      "Squeezed Tensor Shape: torch.Size([5])\n",
      "Squeezed Tensor Shape (dim=0): torch.Size([5, 1, 1])\n",
      "Squeezed Tensor Shape (dim=2): torch.Size([1, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9)\n",
    "\n",
    "# Let's say we want to reshape it to be 3x3\n",
    "x_3x3 = x.view(3, 3)\n",
    "x_3x3 = x.reshape(3, 3)\n",
    "\n",
    "y = x_3x3.t()\n",
    "print(y)\n",
    "\n",
    "### 合并\n",
    "x1 = torch.rand(2, 5)\n",
    "x2 = torch.rand(2, 5)\n",
    "print(torch.cat((x1, x2), dim=0).shape)  # Shape: 4x5\n",
    "print(torch.cat((x1, x2), dim=1).shape)  # Shape 2x10\n",
    "\n",
    "### 改变张量维度顺序的方法\n",
    "batch = 64\n",
    "x = torch.rand((batch, 2, 5))\n",
    "z = x.permute(0, 2, 1)\n",
    "print(x.shape,z.shape)\n",
    "\n",
    "### 添加一个新的维度（轴）\n",
    "x = torch.arange(10)  # Shape is [10], let's say we want to add an additional so we have 1x10\n",
    "print(x.unsqueeze(0).shape)  # 1x10  dim 插入新维度的位置\n",
    "print(x.unsqueeze(1).shape)  # 10x1\n",
    "\n",
    "### 移除张量大小为 1 的维度\n",
    "# 创建一个形状为 (1, 5, 1) 的三维张量\n",
    "x = torch.tensor([[1], [2], [3], [4], [5]]).unsqueeze(0).unsqueeze(2)\n",
    "print(\"Original Tensor Shape:\", x.shape)\n",
    "\n",
    "# 使用 squeeze 移除所有大小为 1 的维度\n",
    "squeezed_tensor = x.squeeze()\n",
    "print(\"Squeezed Tensor Shape:\", squeezed_tensor.shape)\n",
    "# 使用 squeeze 移除第 0 维度上的大小为 1 的维度\n",
    "squeezed_tensor_dim0 = x.squeeze(0)\n",
    "print(\"Squeezed Tensor Shape (dim=0):\", squeezed_tensor_dim0.shape)\n",
    "\n",
    "# 使用 squeeze 移除第 2 维度上的大小为 1 的维度\n",
    "squeezed_tensor_dim2 = x.squeeze(2)\n",
    "print(\"Squeezed Tensor Shape (dim=2):\", squeezed_tensor_dim2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__cuda_array_interface__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_dispatch__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_addmm_activation',\n",
       " '_autocast_to_full_precision',\n",
       " '_autocast_to_reduced_precision',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_conj',\n",
       " '_conj_physical',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_fix_weakref',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_has_symbolic_sizes_strides',\n",
       " '_indices',\n",
       " '_is_all_true',\n",
       " '_is_any_true',\n",
       " '_is_view',\n",
       " '_is_zerotensor',\n",
       " '_lazy_clone',\n",
       " '_make_subclass',\n",
       " '_make_wrapper_subclass',\n",
       " '_neg_view',\n",
       " '_nested_tensor_size',\n",
       " '_nested_tensor_storage_offsets',\n",
       " '_nested_tensor_strides',\n",
       " '_nnz',\n",
       " '_post_accumulate_grad_hooks',\n",
       " '_python_dispatch',\n",
       " '_reduce_ex_internal',\n",
       " '_rev_view_func_unsafe',\n",
       " '_sparse_mask_projection',\n",
       " '_to_dense',\n",
       " '_to_sparse',\n",
       " '_to_sparse_bsc',\n",
       " '_to_sparse_bsr',\n",
       " '_to_sparse_csc',\n",
       " '_to_sparse_csr',\n",
       " '_typed_storage',\n",
       " '_update_names',\n",
       " '_use_count',\n",
       " '_values',\n",
       " '_version',\n",
       " '_view_func',\n",
       " '_view_func_unsafe',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'adjoint',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'aminmax',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctan2_',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_strided_scatter',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_left_shift_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_right_shift_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ccol_indices',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'chalf',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'conj_physical',\n",
       " 'conj_physical_',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'corrcoef',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diagonal_scatter',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dim_order',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'histogram',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_reduce',\n",
       " 'index_reduce_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'ipu',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_conj',\n",
       " 'is_contiguous',\n",
       " 'is_cpu',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_inference',\n",
       " 'is_ipu',\n",
       " 'is_leaf',\n",
       " 'is_maia',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mps',\n",
       " 'is_mtia',\n",
       " 'is_neg',\n",
       " 'is_nested',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xla',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'itemsize',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'mH',\n",
       " 'mT',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'module_load',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mtia',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'nonzero_static',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'register_post_accumulate_grad_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'resize_as_sparse_',\n",
       " 'resolve_conj',\n",
       " 'resolve_neg',\n",
       " 'retain_grad',\n",
       " 'retains_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'row_indices',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'scatter_reduce',\n",
       " 'scatter_reduce_',\n",
       " 'select',\n",
       " 'select_scatter',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slice_inverse',\n",
       " 'slice_scatter',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_padded_tensor',\n",
       " 'to_sparse',\n",
       " 'to_sparse_bsc',\n",
       " 'to_sparse_bsr',\n",
       " 'to_sparse_coo',\n",
       " 'to_sparse_csc',\n",
       " 'to_sparse_csr',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'untyped_storage',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'volatile',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function min in module torch:\n",
      "\n",
      "min(...)\n",
      "    min(input) -> Tensor\n",
      "    \n",
      "    Returns the minimum value of all elements in the :attr:`input` tensor.\n",
      "    \n",
      "    .. warning::\n",
      "        This function produces deterministic (sub)gradients unlike ``min(dim=0)``\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(1, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.6750,  1.0857,  1.7197]])\n",
      "        >>> torch.min(a)\n",
      "        tensor(0.6750)\n",
      "    \n",
      "    .. function:: min(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
      "       :noindex:\n",
      "    \n",
      "    Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum\n",
      "    value of each row of the :attr:`input` tensor in the given dimension\n",
      "    :attr:`dim`. And ``indices`` is the index location of each minimum value found\n",
      "    (argmin).\n",
      "    \n",
      "    If :attr:`keepdim` is ``True``, the output tensors are of the same size as\n",
      "    :attr:`input` except in the dimension :attr:`dim` where they are of size 1.\n",
      "    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\n",
      "    the output tensors having 1 fewer dimension than :attr:`input`.\n",
      "    \n",
      "    .. note:: If there are multiple minimal values in a reduced row then\n",
      "              the indices of the first minimal value are returned.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim (int): the dimension to reduce.\n",
      "        keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (tuple, optional): the tuple of two output tensors (min, min_indices)\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4, 4)\n",
      "        >>> a\n",
      "        tensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n",
      "                [-1.4644, -0.2635, -0.3651,  0.6134],\n",
      "                [ 0.2457,  0.0384,  1.0128,  0.7015],\n",
      "                [-0.1153,  2.9849,  2.1458,  0.5788]])\n",
      "        >>> torch.min(a, 1)\n",
      "        torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))\n",
      "    \n",
      "    .. function:: min(input, other, *, out=None) -> Tensor\n",
      "       :noindex:\n",
      "    \n",
      "    See :func:`torch.minimum`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
