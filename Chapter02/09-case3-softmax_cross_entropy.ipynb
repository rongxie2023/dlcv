{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rongxie2023/dlcv/blob/main/Chapter02//home/xr/dlcv/Chapter02/09-case3-softmax_cross_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 7., 9.]),\n",
       " tensor([[5., 7., 9.]]),\n",
       " tensor([[ 6.],\n",
       "         [15.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0), X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)  ## ***这里未对数据进行处理，避免exp(X)无穷大的情况\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845],\n",
      "        [-1.3986,  0.4033,  0.8380, -0.7193, -0.4033]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6118, 0.0977, 0.0148, 0.2313, 0.0443],\n",
       "         [0.0474, 0.2873, 0.4437, 0.0935, 0.1282]]),\n",
       " tensor([1.0000, 1.0000]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置随机种子以获得可重复的结果\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X = torch.normal(0, 1, (2, 5))\n",
    "print(X)\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845],\n",
      "        [-1.3986,  0.4033,  0.8380, -0.7193, -0.4033]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6118, 0.0977, 0.0148, 0.2313, 0.0443],\n",
       "        [0.0474, 0.2873, 0.4437, 0.0935, 0.1282]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X)\n",
    "nn.Softmax(dim=1)(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 计算交叉熵\n",
    "\n",
    "交叉熵采用真实标签的预测概率的负对数似然。 这里我们不使用Python的for循环迭代预测（这往往是低效的）， 而是通过一个运算符选择所有元素。 下面，我们创建一个数据样本y_hat，其中包含2个样本在3个类别的预测概率， 以及它们对应的标签y。 有了y，我们知道在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。 然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7000, 0.8000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 1])\n",
    "y_hat = torch.tensor([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1]])\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000, 0.2000, 0.1000],\n",
      "        [0.1000, 0.8000, 0.1000]]) tensor([0, 1])\n",
      "tensor([0.3567, 0.2231])\n",
      "tensor([0.7679, 0.6897])\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    y_pred = torch.clip(y_hat, 1e-15, 1 - 1e-15)  # 确保预测值在有效范围内，防止 log(0) 导致无穷大\n",
    "    return - torch.log(y_hat[range(len(y_pred)), y])\n",
    "\n",
    "print(y_hat,y)\n",
    "print(cross_entropy(y_hat, y))\n",
    "print(cross_entropy(softmax(y_hat), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cross Entropy 1 =−(1⋅log(0.7)+0⋅log(0.2)+0⋅log(0.1)) => 0.3567\n",
    "#  Cross Entropy 2 =−(0⋅log(0.1)+1⋅log(0.8)+0⋅log(0.1)) => 0.2231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7288)\n",
      "tensor(0.7679)\n",
      "tensor(0.6897)\n",
      "nll tensor(-0.7500)\n",
      "111 tensor([[0.7000, 0.2000, 0.1000],\n",
      "        [0.1000, 0.8000, 0.1000]]) tensor(0)\n",
      "nll 0 tensor(-0.7000)\n",
      "nll 1 tensor(-0.2000)\n",
      "nll 2 tensor(-0.1000)\n",
      "nll tensor(0.7288)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(criterion(y_hat, y))   # nn.CrossEntropyLoss 对y_hat进行了softmax操作，并计算每条数据的交叉熵损失，并对整个批量的损失进行平均\n",
    "\n",
    "# (0.7679+0.6897)/2 => 0.7288\n",
    "\n",
    "print(criterion(y_hat[0], y[0]))\n",
    "print(criterion(y_hat[1], y[1]))\n",
    "\n",
    "### NLLLoss（Negative Log Likelihood Loss）通常用于处理已经过 LogSoftmax 处理的数据\n",
    "nll_criterion = nn.NLLLoss() \n",
    "print(\"nll\",nll_criterion(y_hat, y) )\n",
    "print(111,y_hat,y[0])\n",
    "print(\"nll 0\",nll_criterion(y_hat[0], torch.tensor(0)))  #nll也是值越小越相似\n",
    "print(\"nll 1\",nll_criterion(y_hat[0], torch.tensor(1)))\n",
    "print(\"nll 2\",nll_criterion(y_hat[0], torch.tensor(2)))\n",
    "\n",
    "print(\"nll\",nll_criterion(torch.log(softmax(y_hat)), y) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real3dportrait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
